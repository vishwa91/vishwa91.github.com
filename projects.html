<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Personal website">
    <meta name="Vishwanath | Publications" content="">
    <!-- link rel="icon" href="../../../../favicon.ico" -->

    <title>Vishwanath | Projects</title>

    <!-- Bootstrap core CSS -->
    <link href="bootstrap/css/bootstrap.css" rel="stylesheet">

    <!-- Font awesome and academicons -->
    <link href="font-awesome/css/font-awesome.css" rel="stylesheet">
    <link href="academicons/css/academicons.css" rel="stylesheet">
  </head>

  <body>
    <!-- Navbar -->
    <div class="container">
          <nav class="navbar navbar-expand-lg navbar-light bg-white">
          <a class="navbar-brand" href="#">Vishwanath's website</a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav">
              <li class="nav-item">
                <a class="nav-link" href="index.html">Home</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="projects.html">Projects</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="publications.html">Publications</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="misc.html">Medley</a>
              </li>
              <li class="nav-item dropdown">
                  <a class="nav-link dropdown-toggle" href="#" id="HobbyMenu" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
            Musings</a>
                <div class="dropdown-menu" aria-labelledby="HobbyMenu">
                  <a class="dropdown-item" href="art.html">Art and craft</a>
                  <a class="dropdown-item" href="biking.html">Biking</a>
                  <a class="dropdown-item" href="https://cyroforge.wordpress.com">Blog</a>
                </div>
              </li>
            </ul>
          </div>
        </nav>
        <hr>
    </div>

    <div class="container">
        <h1>Projects</h1>
        A comprehensive list of major projects I worked on as part of graduate and undergraduate studies.
        <p style="padding: 1em", id="programmable"></p>
        <h2>Material Classification with Spectrally-Programmable Camera (ICCP)</h2>
        <p style="padding: 0.25em"></p>
        <div class="row">
            <div class="col-sm-4">
              <figure class="figure text-center">
                <img src="files/research/iccp2020_1.png" class="figure-img rounded img-fluid">
                <figcaption class="figure-caption text-justify">Material classification with spectrum often requires a linear transformation, which can be optically implemented as spectral filtering</figcaption>
			          <br>
			          <img src="files/research/iccp2020_2.png" class="figure-img rounded img-fluid">
	              <figcaption class="figure-caption text-justify">We build a material classification camera that can perform per-pixel labeling at very high spatial resolution and with as few as two images for binary classification.</figcaption>
	            </figure>
	          </div>
	          <div class="col-sm-8">
	            <div class="row">
	              <span class="col-12"><p class="text-justify">
		              Materials can be reliably classified by measuring their spectrum. Classification of materials in a scene can then by done by capturing spectrum at each pixel, 
		              resulting in a hyperspectral image, a measurement process that is lengthy and inherently wasteful.
                <br>
                <br>
                  We rely on a simple observation that most spectral classification tasks require computing a linear transform of the spectrum, and the number of such features are very small in number, such as a single measurement for binary classification. We optically compute this linear transformation by building a spectrally-programmable camera,
            and capturing images with discriminant spectral filters. this leads to capturing a small set of images, and hence efficiently, high frame-rate material classification setup.
                </p>
                </span>
              </div>
              <div class="row">
              <div class="col-sm-2">
                  <span class="fa fa-file-pdf-o fa-2x"></span>  <a href="files/papers/vishwa_iccp2020.pdf">Paper</a>
              </div>
              <div class="col-sm-2">
                  <span class="fa fa-github fa-2x"></span>  <a href="https://github.com/image-science-lab/programmable-spectrometry">Code</a>
              </div>
              <!--div class="col-sm-2">
                  <span class="fa fa-file-powerpoint-o fa-2x"></span>  <a href="files/presentations/vishwa_krism2017.pdf">Ppt</a>
              </div-->
              <div class="col-sm-8">
                <p>
                  <button class="btn btn-info" type="button" data-toggle="collapse" data-target="#cite8" aria-expanded="false" aria-controls="cite8">
                      Cite
                  </button>
                  <div class="collapse" id="cite8">
                    <div class="card card-block small">
                      @article{saragadam2020programmable, <br>
                      title={Programmable Spectrometry: Per-Pixel Material Classification using Learned Spectral Filters}, <br>
                      author={Saragadam, Vishwanath and Sankaranarayanan, Aswin}, <br>
                      journal={IEEE Intl. Conf. Computational Photography (ICCP)}, <br>
                      year={2020}
                      }
                    </div>
                  </div>
                </p>
              </div>
            </div>

          </div>
          <p style="margin: 1em;"></p>
        </div>
        <hr>

	      <p style="padding: 1em", id="uncertainty"></p>
        <h2>On Space-Spectrum Uncertainty of Spectrally-Programmable Cameras (Optics Express)</h2>
        <p style="padding: 0.25em"></p>
        <div class="row">
            <div class="col-sm-6">
              <figure class="figure text-center">
                <img src="files/research/osa2020_1.png" class="figure-img rounded img-fluid">
                <figcaption class="figure-caption text-justify">Spectrally-programmable cameras that rely on a spatial light modulator on the rainbow plane, are directly affected by size of the aperture. We show that the 
                  spectral and spatial resolution are inversely proportional and the product can be lower bounded.
                </figure>
	          </div>
	          <div class="col-sm-6">
	            <div class="row">
	              <span class="col-12"><p class="text-justify">
                  The most efficient way of implementing spectral-programming is to rely on a setup that places a diffraction grating on the image plane, which then generates
                  the so-called rainbow plan. A spatial light modulator (SLM) can then be placed on this rainbow plane to achieve arbitrary spectral coding.
                  Such a setup requires a narrow slit to implement sharp spectral filters, but leads to blurry images. A wide aperture, in contrast achieves high
                  spatial resolution but loses spectral resolution.
                <br>
                <br>
                  In this work, we showed that this space and spectrum trade off can be theoretically be represented as a space-spectrum bandwidth product. We proved that 
                  the product of standard deviations of spatial and spectral blurs can be lower bounded by <a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\lambda}{4\pi\nu_0}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\lambda}{4\pi\nu_0}" title="\frac{\lambda}{4\pi\nu_0}" /></a>
                  femto square-meters, where <a href="https://www.codecogs.com/eqnedit.php?latex=\nu_0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\nu_0" title="\nu_0" /></a> is the density of
                  groves in the diffraction grating and <a href="https://www.codecogs.com/eqnedit.php?latex=\lambda" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\lambda" title="\lambda" /></a> is the central
                  wavelength of light.
                </p>
                </span>
              </div>
              <div class="row">
              <div class="col-sm-2">
                  <span class="fa fa-file-pdf-o fa-2x"></span>  <a href="files/papers/vishwa_osa2020.pdf">Paper</a>
              </div>
              <!--div class="col-sm-2">
                  <span class="fa fa-file-powerpoint-o fa-2x"></span>  <a href="files/presentations/vishwa_krism2017.pdf">Ppt</a>
              </div-->
              <div class="col-sm-8">
                <p>
                  <button class="btn btn-info" type="button" data-toggle="collapse" data-target="#cite7" aria-expanded="false" aria-controls="cite7">
                      Cite
                  </button>
                  <div class="collapse" id="cite7">
                    <div class="card card-block small">
                      @article{saragadam2020uncertainty, <br>
                      title={On Space-Spectrum Uncertainty Analysis for Spectrally-Programmable Cameras}, <br>
                      author={Saragadam, Vishwanath and Sankaranarayanan, Aswin}, <br>
                      journal={OSA Optics Express}, <br>
                      year={2020}
                      }
                    </div>
                  </div>
                </p>
              </div>
            </div>

          </div>
          <p style="margin: 1em;"></p>
        </div>
        <hr>

	<p style="padding: 1em", id="krism"></p>
        <h2>Optical Singular Value Decomposition for Hyperspectral Imaging <br>(Trans. Graphics)</h2>
        <p style="padding: 0.25em"></p>
        <div class="row">
            <div class="col-sm-4">
              <figure class="figure text-center">
                <img src="files/research/krism2019_1.png" class="figure-img rounded img-fluid">
                <figcaption class="figure-caption text-justify">We propose an optical-computing method for capturing low-rank approximation of a scene's hyperspectral image.</figcaption>
			<br>
			<img src="files/research/krism2019_2.png" class="figure-img rounded img-fluid">
	<figcaption class="figure-caption text-justify">Our setup is capable of capturing very high spatial and spectral resolution with 20x fewer measurements than Nyquist rate.</figcaption>
	</figure>
	</div>
	<div class="col-sm-8">
	<div class="row">
	<span class="col-12"><p class="text-justify">
		Hyperspectral cameras capture images across several, narrowband wavelengths, which finds usage in numerous computer vision and material identification 
		applications. Due to a dense sampling of space and spectrum, the captured hyperspectral image is often very high dimensional. This leads to severe loss in
		SNR per band, requires very long exposure times and is inherently wasteful.
		<br>
		<br>
		
		A key observation is that a hyperspectral image of a natural scene has very limited spectral diversity, leading to a concise low-rank representation.
		We propose an optical imager that directly captures this low-rank subspace. We achieve this by implementing two optical operators -- a spatially-coded
		spectrometer and a spectrally-coded imager. By alternating between the two, using output of one operator as input to other operator, we capture a low-rank
		approximation with as few as 10 measurements.
	</p></span>
        </div>
            <div class="row">
              <div class="col-sm-2">
                  <span class="fa fa-file-pdf-o fa-2x"></span>  <a href="files/papers/vishwa_krism2018.pdf">Paper</a>
              </div>
              <!--div class="col-sm-2">
                  <span class="fa fa-file-powerpoint-o fa-2x"></span>  <a href="files/presentations/vishwa_krism2017.pdf">Ppt</a>
              </div-->
              <div class="col-sm-8">
                <p>
                  <button class="btn btn-info" type="button" data-toggle="collapse" data-target="#cite6" aria-expanded="false" aria-controls="cite6">
                      Cite
                  </button>
                  <div class="collapse" id="cite6">
                    <div class="card card-block small">
                      @article{saragadam2018krism, <br>
                      title={KRISM: Krylov Subspace-based Optical Computing of Hyperspectral Images}, <br>
                      author={Saragadam, Vishwanath and Sankaranarayanan, Aswin}, <br>
                      journal={ACM Trans. Graphics}, <br>
                      year={2019}
							        }
                    </div>
                  </div>
                </p>
              </div>
            </div>

          </div>
          <p style="margin: 1em;"></p>
        </div>
        <hr>
		
		    <p style="padding: 1em", id="msl"></p>
        <h2>Micro-baseline Structured Light (ICCV)</h2>
        <p style="padding: 0.25em"></p>
        <div class="row">
            <div class="col-sm-4">
              <figure class="figure text-center">
                <img src="files/research/msl2019_1.png" class="figure-img rounded img-fluid">
                <figcaption class="figure-caption text-justify">We propose a fast and accurate SL technique that utilizes narrow (micro) baseline to linearize the SL equation.</figcaption>
				<br>
				<img src="files/research/msl2019_2.png" class="figure-img rounded img-fluid">
                <figcaption class="figure-caption text-justify">Our technique is capable of computing accurate depth map with simple, local least squares technique.</figcaption>
              </figure>
            </div>
            <div class="col-sm-8">
              <div class="row">
                <span class="col-12"><p class="text-justify">
				Structured Light (SL) relies on projecting a known pattern and capturing an image of the scene. By computing the correspondences between projector and camera pixels, SL
				is capable of highly accurate depth map estimation. Existing SL techniques either require projection of multiple patterns, or rely on complex computing to estimate
				the depth map, both of which preclude an efficient implementation on mobile systems such as cellphones and drones.
				<br>
				<br>
				
				Devices with small real estates can only accommodate a narrow (micro) baseline between camera and projector. We observe that such a narrow baseline can be
				used for linearizing the otherwise non-linear SL equation relating projected pattern and captured image. This leads to a linear equation in two unknowns (albedo and disparity)
				at each pixel. The resulting equation can then be efficiently solved using a local least-squares approach which requires <b>minimal computational resources</b>, and 
				needs <b>projection of only a single pattern</b>.
                </p></span>
              </div>
            <div class="row">
              <div class="col-sm-4">
                  <span class="fa fa-file-pdf-o fa-2x"></span>  <a href="files/papers/vishwa_iccv2019.pdf">Paper</a>
              </div>
              <!--div class="col-sm-2">
                  <span class="fa fa-file-powerpoint-o fa-2x"></span>  <a href="files/presentations/vishwa_krism2017.pdf">Ppt</a>
              </div-->
              <div class="col-sm-8">
                <p>
                  <button class="btn btn-info" type="button" data-toggle="collapse" data-target="#cite5" aria-expanded="false" aria-controls="cite5">
                      Cite
                  </button>
                  <div class="collapse" id="cite5">
                    <div class="card card-block small">
							 @inproceedings{saragadam2019msl, <br>
							title={Micro-baseline Structured Light}, <br>
							author={Saragadam, Vishwanath and Wang, Jian and Gupta, Mohit and Nayar, Shree}, <br>
							booktitle={Intl. Conf. Computer Vision}, <br>
							year={2019}, 
							}
                    </div>
                  </div>
                </p>
              </div>
            </div>

          </div>
          <p style="margin: 1em;"></p>
        </div>
        <hr>
		

        <p style="padding: 1em", id="wavelet"></p>
        <h2>Wavelet tree parsing with freeform lensing (ICCP)</h2>
        <p style="padding: 0.25em"></p>
        <div class="row">
            <div class="col-sm-4">
              <figure class="figure text-center">
                <img src="files/research/iccp2019_1.png" class="figure-img rounded img-fluid">
                <figcaption class="figure-caption text-justify">Wavelet tree parsing with traditional single pixel camera vs. freeform lensing. Concentrating
                light in the compact wavelet pattern guarantees a high SNR independent of spatial scales. This results in high quality images with practically
                no reconstruction time with significantly small number of measurements.</figcaption>
              </figure>
            </div>
            <div class="col-sm-8">
              <div class="row">
                <span class="col-12"><p class="text-justify">Wavelet transform of an image is both a sparsifying and a predictive transformation. By optically measuring the wavelet coefficients with a single pixel camera that measures linear projections of scene's image, one can adaptively tease out the dominant wavelet coefficients, requiring fewer measurements than compressive sensing. In practice, such a method faces the debilitating problem of increasing noise with increasing spatial scales, due to spatially compact wavelet basis.<br>
                <br>

                Instead of using a DMD as a spatial light modulator (SLM), we propose using a phase-only SLM which simply redistributes light into the spatially compact
                basis, thus maintaining a constant measurement SNR independent of spatial scale. This allows high quality imaging with a small set of high SNR measurements made adaptively.
                <br>
                <br>
                  </p></span>
              </div>
            <div class="row">
              <div class="col-sm-2">
                  <span class="fa fa-file-pdf-o fa-2x"></span>  <a href="files/papers/vishwa_iccp2019.pdf">Paper</a>
              </div>
              <!--div class="col-sm-2">
                  <span class="fa fa-file-powerpoint-o fa-2x"></span>  <a href="files/presentations/vishwa_iccp2017.pdf">Ppt</a>
              </div-->
              <div class="col-sm-8">
                <p>
                  <button class="btn btn-info" type="button" data-toggle="collapse" data-target="#cite4" aria-expanded="false" aria-controls="cite4">
                      Cite
                  </button>
                  <div class="collapse" id="cite4">
                    <div class="card card-block small">
                      @inproceedings{saragadam2019wavelet,<br>
                        title={Wavelet parsing with freeform lensing},<br>
                        author={Saragadam, Vishwanath and Sankaranarayanan, Aswin C},<br>
                        booktitle={Intl. Conf. Computational Photography},<br>
                        pages={1--9},<br>
                        year={2019},<br>
                        organization={IEEE}<br>
                      }
                    </div>
                  </div>
                </p>
              </div>
            </div>

          </div>
          <p style="margin: 1em;"></p>
        </div>
        <hr>


        <p style="padding: 1em", id="anomaly"></p>
        <h2>Compressive spectral anomaly detection (ICCP)</h2>
        <p style="padding: 0.25em"></p>
        <div class="row">
            <div class="col-sm-4">
              <figure class="figure text-center">
                <img src="files/research/iccp2017_1.png" class="figure-img rounded img-fluid">
                <img src="files/research/iccp2017_2.png" class="figure-img rounded img-fluid">
                <figcaption class="figure-caption text-justify">Detection of anomalies via compressive hyperspectral camera. Contrary to existing methods,
                our method detects anomalies faster and with far fewer measurements.</figcaption>
              </figure>
            </div>
            <div class="col-sm-8">
              <div class="row">
                <span class="col-12"><p class="text-justify">One of the distinguishing feature of materials is their spectral signature, which is the intensity of light the material reflects as a function
                of wavelength. Hyperspectral images, which capture spectral signataure at every pixel, are used for material classification and one specific
                application is anomaly detection, where materials very different from the background and in trace quantities are identified.<br>
                <br>

                However, capturing all the data to detect material present in trace quantities is both costly and wasteful. Instead, we propose a novel
                two stage procedure where we first identify the spectrum of the background and remove it. In the second stage, due to absence of background,
                presence of anomalies can be seen as measurement of a sparse signal, which can be done with various compressive sensing techniques.<br>
                <br>
                  </p></span>
              </div>
            <div class="row">
              <div class="col-sm-2">
                  <span class="fa fa-file-pdf-o fa-2x"></span>  <a href="files/papers/vishwa_iccp2017.pdf">Paper</a>
              </div>
              <div class="col-sm-2">
                  <span class="fa fa-file-powerpoint-o fa-2x"></span>  <a href="files/presentations/vishwa_iccp2017.pdf">Ppt</a>
              </div>
              <div class="col-sm-8">
                <p>
                  <button class="btn btn-info" type="button" data-toggle="collapse" data-target="#cite3" aria-expanded="false" aria-controls="cite3">
                      Cite
                  </button>
                  <div class="collapse" id="cite3">
                    <div class="card card-block small">
                      @inproceedings{saragadam2017compressive,<br>
                        title={Compressive spectral anomaly detection},<br>
                        author={Saragadam, Vishwanath and Wang, Jian and Li, Xin and Sankaranarayanan, Aswin C},<br>
                        booktitle={Computational Photography (ICCP), 2017 IEEE International Conference on},<br>
                        pages={1--9},<br>
                        year={2017},<br>
                        organization={IEEE}<br>
                      }
                    </div>
                  </div>
                </p>
              </div>
            </div>

          </div>
          <p style="margin: 1em;"></p>
        </div>
        <hr>

        <p style="padding: 1em", id="crosscale"></p>
        <h2>Cross-scale predictive dictionaries (Trans. Image Processing)</h2>
        <p style="padding: 0.25em"></p>
        <div class="row">
            <div class="col-sm-4">
              <figure class="figure text-center">
                <img src="files/research/tip2016_1.png" class="figure-img rounded img-fluid" style="width: 45%">
                <img src="files/research/tip2016_2.png" class="figure-img rounded img-fluid" style="width: 45%">
                <figcaption class="figure-caption text-justify">Speed up obtained for compressive sensing of videos by multi-scale and predictive overcomplete dictionaries.
                By embedding wavelet-tree like structure into overcomplete dictionaries, we can obtain 13x-100x speedups.</figcaption>
              </figure>
            </div>
            <div class="col-sm-8">
              <div class="row">
                <span class="col-12"><p class="text-justify">Contrary to an orthonormal basis, an overcomplete dictionaries has far more elements than the signal dimension,
                  which admits sparse representation. Such a framework enables compressive sensing tasks, by exploiting the sparsity during sensing
                  as well as recovery of the signal.<br>
                <br>

                Unfortunately, such applications are far from practical, due to the enormous dictionaries required for good results and the significant
                time required for recovery. We instead propose a novel clustering technique which reduces the recovery time by a factor of 10x to 100x,
                with very small loss in accuracy. The clustering of dictionary elements is done by identifying that visual signals are similar across
                scales, thus enabling identification of clusters with a down-sampled version of the same signal.<br>
                <br>
                  </p></span>
              </div>
            <div class="row">
              <div class="col-sm-2">
                  <span class="fa fa-file-pdf-o fa-2x"></span>  <a href="files/papers/vishwa_arxiv2016.pdf">Paper</a>
              </div>
              <div class="col-sm-2">
                  <span class="fa fa-file-powerpoint-o fa-2x"></span>  <a href="files/presentations/vishwa_icip2016.pdf">Ppt</a>
              </div>
              <div class="col-sm-2">
                  <span class="fa fa-github fa-2x"></span>  <a href="https://github.com/image-science-lab/zero-tree_OMP">Code</a>
              </div>
              <div class="col-sm-6">
                <p>
                  <button class="btn btn-info" type="button" data-toggle="collapse" data-target="#cite2" aria-expanded="false" aria-controls="cite2">
                      Cite
                  </button>
                  <div class="collapse" id="cite2">
                    <div class="card card-block small">
                      @article{saragadam2015cross,<br>
                      title={Cross-scale predictive dictionaries},<br>
                      author={Saragadam, Vishwanath and Sankaranarayanan, Aswin and Li, Xin},<br>
                      journal={arXiv preprint arXiv:1511.05174},<br>
                      year={2015}<br>
                      }
                    </div>
                  </div>
                </p>
              </div>

            </div>

          </div>
          <p style="margin: 1em;"></p>
        </div>
        <hr>

        <p style="padding: 1em"></p>
        <h2>Utilizing motion sensors for some image processing applications</h2>
        <p style="padding: 0.25em"></p>
        <div class="row">
            <div class="col-sm-4">
              <figure class="figure text-center">
                <img src="files/research/btp1.png" class="figure-img rounded img-fluid" style="width: 45%">
                <img src="files/research/btp2.png" class="figure-img rounded img-fluid" style="width: 45%">
                <figcaption class="figure-caption text-justify">Estimating depth using motion sensors from a camera. Along with a control over camera
                settings, a cellphone camera offers inertial motion sensing data which can be used for various image processing tasks.</figcaption>
              </figure>
            </div>
            <div class="col-sm-8">
              <div class="row">
                <span class="col-12"><p class="text-justify">Mobiles have become ubiquitous over the past decade, and they offer more data than a traditional DSLR can
                offer, such as accelerometer, compass and gyroscopes. At the same time, human held mobiles tend to create a blur due to hand shake. <br>
                <br>

                We proposed some novel applications of such mobile imagery, where we incorporate the sensor information for various image processing
                tasks such as depth estimation, image deblurring, all-focus image and so on. This work was part of my undergrad thesis where
                I worked with <a href="http://www.ee.iitm.ac.in/~raju/">Prof. A N Rajagopalan</a><br>
                <br>
                  </p></span>
              </div>
            <div class="row">
              <div class="col-sm-2">
                  <span class="fa fa-file-pdf-o fa-2x"></span>  <a href="files/vishwa_undergrad_thesis.pdf">Thesis</a>
              </div>
              <div class="col-sm-2">
                <span class="fa fa-github fa-2x"></span> <a href="https://github.com/vishwa91/mobile_deconv">Code</a>
              </div>
              <div class="col-sm-8">
                <p>
                  <button class="btn btn-info" type="button" data-toggle="collapse" data-target="#cite1" aria-expanded="false" aria-controls="cite1">
                      Cite
                  </button>
                  <div class="collapse" id="cite1">
                    <div class="card card-block small">
                      @phdthesis{saragadam2014utilizing,<br>
                      title={Utilizing motion sensors for some image processing applications},<br>
                      author={Saragadam, Vishwanath and Rajagopalan, A. N},<br>
                      year={2014},<br>
                      school={Indian Institute of Technology Madras}<br>
                      }
                    </div>
                  </div>
                </p>
              </div>


            </div>

          </div>
          <p style="margin: 1em;"></p>
        </div>
        <hr>

    </div>

    <footer class="footer">
     <div class="container">
         <span class="text-muted">&copy; Vishwanath Saragadam 2019</span>
     </div>
    </footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
    <script src="bootstrap/js/bootstrap.js"></script>
  </body>
</html>
